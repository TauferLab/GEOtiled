"""
GEOtiled Refactored Library v1.0.0
GCLab 2024

Compiled by Jay Ashworth (@washwor1) and Gabriel Laboy (@glaboy-vol)

Derived from original work by: Camila Roa (@CamilaR20), Eric Vaughan (@VaughanEric), Andrew Mueller (@Andym1098), Sam Baumann (@sam-baumann), David Huang (@dhuang0212), and Ben Klein (@robobenklein)

Learn more about GEOtiled from the paper: https://dl.acm.org/doi/pdf/10.1145/3588195.3595941
"""

from osgeo import osr, ogr, gdal
from pathlib import Path
from tqdm import tqdm

import matplotlib.pyplot as plt
import geopandas as gpd
import pandas as pd
import numpy as np

import concurrent.futures
import multiprocessing
import subprocess
import requests
import zipfile
import shutil
import math
import glob
import json
import os
import re

# CONSTANTS
TEXT_FILE_EXTENSION = ".txt"
GEOTIFF_FILE_EXTENSION = ".tif"
VRT_DEFAULT_FILE_NAME = "merged.vrt"
SHAPEFILE_FOLDER_NAME = "shapefiles"

# Used to silence a deprecation warning. 
gdal.UseExceptions()

class GEOtiled:
    """
    A class to represent the GEOtiled workflow.

    Attributes
    ----------
    data_dir : str
        The directory where data generated is stored (default is ./)
    """
    
    def __init__(self, data_dir="./"):
        """
        Initialization of class.

        The init function handles creation of the passed `data_dir` for storing data.
        
        Parameters
        ----------
        data_dir : str
            The directory where generated data is stored (default is ./)

        Raises
        ------
        ValueError
            If the base path of passed `data_dir` doesn't exist, it cannot create the new directory.
        """

        # Check to ensure base path exists for given data directory
        if os.path.exists(os.path.dirname(data_dir)):
            Path(data_dir).mkdir(parents=True, exist_ok=True)
        else:
            err_str = "Base path " + os.path.dirname(data_dir) + " doesn't exist. Cannot initialize object."
            raise ValueError(err_str)
        
        self.data_dir = data_dir

    
    def get_data_directory(self):
        """
        Returns the current data directory

        Allows the user to see what directory data is being stored in.
        """

        print("The current data directory is", self.data_dir)

    
    def set_data_directory(self, path):
        """
        Changes the working directory.

        Allows the user to change the data directory where data generated by GEOtiled will be stored.
        It will also create the directory for the user if it doesn't already exist, but the base path
        must exist.

        Parameters
        ----------
        path : str
            Directory to store data in.
        """

        if os.path.exists(os.path.dirname(path)):
            Path(path).mkdir(parents=True, exist_ok=True)
            self.data_dir = path
        else:
            print("Base path", os.path.dirname(path), "doesn't exist. Cannot set data directory.")


    def __get_codes(self, code_type):
        """
        Reads code data from selected file and returns data as JSON dictionary.

        Parameters
        ----------
        code_type : str
            Specifies codes for which data are desired.
        """

        # Build path to text file containing region codes
        path_suffix = "codes/" + code_type + "_codes.txt"
        codes_path = os.path.join(os.path.dirname(os.path.realpath(__file__)), path_suffix)
    
        # Read in JSON formatted codes
        data = None
        with open(codes_path) as f: 
            data = f.read() 
        codes = json.loads(data) 

        return codes


    def print_data_codes(self):
        """
        Print contents of 'data_codes.txt'.
    
        Outputs codes and their correlating terrain paramter from the 'data_codes.txt' file.
        These codes are meant to inform the user what elevation data is available for download off the USGS webpage.
        """
    
        # Read in JSON formatted codes
        data_codes = self.__get_codes("data")
    
        # Print all codes and their associated parameter
        print("The data codes and their associated parameter are:")
        for key in data_codes:
            print(key, ":", data_codes[key])

    
    def print_parameter_codes(self, param=None):
        """
        Print contents of 'parameter_codes.txt'.
    
        Outputs codes and their correlating terrain paramter from the 'parameter_codes.txt' file.
        These codes are meant to inform the user what parameters are available for computation.
    
        Parameters
        ----------
        param : str, optional
            A terrain parameter to return the code for (default is None).
        """
    
        # Read in JSON formatted codes
        param_codes = self.__get_codes("parameter")
    
        # Output requested info
        if param is not None:
            param = param.lower()
            if param not in param_codes.values():
                # If the parameter doesn't exist, inform the user
                print("The entered parameter was not found.")
            else:
                # Print parameter and correlating code
                code_list = list(param_codes.keys())
                param_pos = list(param_codes.values()).index(param)
                print("The code for", param, "is", code_list[param_pos])
        else:
            # Print all codes and their associated parameter
            print("The parameter codes and their associated parameter are:")
            for key in param_codes:
                print(key, ":", param_codes[key])
    
    
    def __extract_region_from_link(self, link):
        """
        Extract name of region from download link.
    
        The function pulls the name of the region from USGS download links for shapefiles.
    
        Parameters
        ----------
        link : str
            USGS specific link that specifies URL to download a shapefile.
    
        Returns
        -------
        str
            Region name extracted from the link.
        """
        
        re_link = "https://prd-tnm.s3.amazonaws.com/StagedProducts/GovtUnit/Shape/GOVTUNIT_(.*)_State_Shape.zip"
        region = re.search(re_link, link).group(1).replace("_", " ") # Extract region from link
        return region
    
    
    def print_region_codes(self, region=None):
        """
        Print contents of 'region_codes.txt'.
    
        Outputs codes and their correlating region from the 'region_codes.txt' file.
        These code are meant to inform the user what shapefiles are available for download.
    
        Parameters
        ----------
        region : str, optional
            A region to return the code for (default is None). Not case-sensitive.
        """
    
        # Read in JSON formatted codes
        region_codes = self.__get_codes("region")
    
        # Output requested info
        if region is not None:
            region = region.lower()
    
            # Extract all regions and put them in a temp dictionary
            extracted_regions = {}
            for reg in region_codes.values(): 
                extracted_regions.update({self.__extract_region_from_link(reg).lower():reg})
                
            if region not in extracted_regions.keys():
                # If the code doesn't exist, inform the user
                print("The entered code was not found.")
            else:
                # Extract region from link and print
                code_list = list(region_codes.keys())
                reg_pos = list(region_codes.values()).index(extracted_regions[region])
                print("The code for", region, "is", code_list[reg_pos])
        else:
            # Print all codes and their associated region
            print("The region codes and their associated region are:")
            for key in region_codes:
                region = self.__extract_region_from_link(region_codes[key])
                print(key, ":", region)
    
        
    def __bash(self, argv):
        """
        Executes a command in bash.

        This function acts as a wrapper to execute bash commands using the Python subprocess Popen method. 
        Commands are executed synchronously, stdout and stderr are captured, and errors can be raised.

        Parameters
        ----------
        argv : list
            List of arguments for a bash command. They should be in the order that you would arrange them in the command line (e.g., ["ls", "-lh", "~/"]).

        Raises
        ------
        RuntimeError
            Popen returns with an error if the passed bash function returns an error.
        """

        arg_seq = [str(arg) for arg in argv] # Convert all arguments in list into a string
        proc = subprocess.Popen(arg_seq, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
        proc.wait() # synchronize
        stdout, stderr = proc.communicate()
    
        # Print error message if execution returned error
        if proc.returncode != 0:
            raise RuntimeError("'%s' failed, error code: '%s', stdout: '%s', stderr: '%s'" % (
                ' '.join(arg_seq), proc.returncode, stdout.rstrip(), stderr.rstrip()))

    
    def __get_file_size(self, url):
        """
        Retrieve the size of a file at a given URL in bytes.
    
        This function sends a HEAD request to the provided URL and reads the 'Content-Length' header to determine the size of the file. 
        It's primarily designed to support the `download_files` function to calculate download sizes beforehand.
    
        Parameters
        ----------
        url : str
            The download URL to determine the file size from.
    
        Returns
        -------
        int
            Size of the file at the specified URL in bytes. Returns 0 if the size cannot be determined.
        """
        
        try:
            response = requests.head(url)
            return int(response.headers.get("Content-Length", 0))
        except:
            return 0


    def __download_file(self, url, output_folder, pbar):
        """
        Download a single file from a URL and store it in a specified folder.
    
        This is a utility function that facilitates the downloading of files, especially within iterative download operations.
        If the file already exists, skip the download.
    
        Parameters
        ----------
        url : str
            The download URL of the file.
        output_folder : str
            The path where the downloaded file will be stored.
        pbar : tqdm object
            Reference to the tqdm progress bar, typically used in a parent function to indicate download progress.
    
        Returns
        -------
        int
            The number of bytes downloaded.
        """

        # Determine if file already exists
        output_file = os.path.join(output_folder, url.split('/')[-1])
        if os.path.exists(output_file):
            return 0

        # Get download contents
        response = requests.get(url, stream=True)

        # Write contents of download to file
        downloaded_size = 0
        with open(output_file, "wb") as f:
            for chunk in response.iter_content(chunk_size=8192):
                f.write(chunk)
                downloaded_size += len(chunk)
                pbar.update(len(chunk)) # Update progress bar
                
        return downloaded_size


    def download_files(self, download_list, download_folder):
        """
        Download file(s) from provided URL(s) and store them in a desired folder.
    
        This function allows for the simultaneous downloading of files using threading
        and showcases download progress via a tqdm progress bar.
    
        Parameters
        ----------
        download_list : str, List[str]
            The name of a text file. This file should contain download URLs separated by newlines.
            A list of download URLs.
        download_folder : str
            Name of the folder where the downloaded files will be stored.
        """
        
        # Create directory to store downloaded files in
        output_folder = os.path.join(self.data_dir, download_folder)
        Path(output_folder).mkdir(parents=True, exist_ok=True)
    
        # Determine if URLs are raw list or from a text file
        if type(download_list) is not list:
            download_list = os.path.join(self.data_dir, download_list + TEXT_FILE_EXTENSION) # Update list to be full path to text file
            with open(download_list, "r", encoding="utf8") as dsvfile:
                urls = [url.strip().replace("'$", "")
                        for url in dsvfile.readlines()]
        else:
            urls = download_list
        
        # Compute total size of files to download
        total_size = sum(self.__get_file_size(url.strip()) for url in urls)
    
        # Create progress bar to track completion of downloads
        with tqdm(total=total_size, unit="B", unit_scale=True, ncols=100, desc="Downloading", colour="green") as pbar:
            with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:
                futures = [executor.submit(self.__download_file, url, output_folder, pbar) for url in urls]
                for future in concurrent.futures.as_completed(futures):
                    size = future.result()


    def __download_shapefiles(self, codes):
        """
        Download shapefile(s) from a list of specified codes.
        
        This function downloads specified shapefile(s) off the USGS webpage and stores them in a 'shapefiles' folder located in the data directory.
        All codes passed should be valid codes from the 'region_codes.txt' file.
        It will skip downloading already existing shapefiles.
    
        Parameters
        ----------
        codes : str, List[str]
            Comma-separated string of valid codes.
            List of valid codes.
        """
        
        # Turn codes into list if not one
        if not isinstance(codes, list): 
            codes = list(codes.split(","))
        
        # Ensure codes passed are valid codes
        region_codes = self.__get_codes("region")
        for code in codes:
            if code not in list(region_codes.keys()):
                print(code, "is not a valid region code. Terminating execution.")
                return
        
        # Create path to shape file folder
        shape_path = os.path.join(self.data_dir, SHAPEFILE_FOLDER_NAME)
    
        # Iterate through all passed code
        download_links = []
        download_codes = []
        for code in codes:
            # Create path to shape file
            shapefile_path = os.path.join(shape_path, code, code + ".shp")
    
            # Check if doesn't exist, add download URL to list
            if not os.path.isfile(shapefile_path):
                # Get download URL based off region code
                download_links.append(region_codes[code])
                download_codes.append(code)
    
        # Do the downloads if the list contains URLs
        if len(download_links) > 0:
            self.download_files(download_links, SHAPEFILE_FOLDER_NAME)
    
            # Unzip and rename files
            for iter, link in enumerate(download_links):
                # Isolate zip file name
                zip_name = os.path.basename(link)
    
                # Update path to zip file
                zip_path = os.path.join(shape_path, zip_name)
    
                # Unzip and get correct region files
                original_file_name = "GU_StateOrTerritory"
                original_file_path = "Shape/" + original_file_name
                with zipfile.ZipFile(zip_path, "r") as file:
                    file.extract(original_file_path+".dbf", path=shape_path)
                    file.extract(original_file_path+".prj", path=shape_path)
                    file.extract(original_file_path+".shp", path=shape_path)
                    file.extract(original_file_path+".shx", path=shape_path)
                
                file.close()
                
                # Rename folder and files
                dwnld_code = download_codes[iter]
                new_directory = os.path.join(shape_path, dwnld_code)
                new_dir_with_orig = os.path.join(new_directory, original_file_name)
                new_dir_with_code = os.path.join(new_directory, dwnld_code)
                os.rename(os.path.join(shape_path, "Shape"), new_directory)
                os.rename(new_dir_with_orig+".dbf", new_dir_with_code+".dbf")
                os.rename(new_dir_with_orig+".prj", new_dir_with_code+".prj")
                os.rename(new_dir_with_orig+".shp", new_dir_with_code+".shp")
                os.rename(new_dir_with_orig+".shx", new_dir_with_code+".shx")
    
                # Delete original zip file
                os.remove(zip_path)


    def __get_extents(self, shapefile):
        """
        Returns the bounding box (extents) of a shapefile.
        
        This function extracts the extents of a shapefile. The extent is the upper left and lower right coordinates of the file.
        If the shapefile passed in isn't already download it, this function will do it automatically.
    
        Parameters
        ----------
        shapefile : str
            Code of shapefile to get extents for.
        
        Returns
        -------
        Tuple(Tuple(float))
            Returns two tuples - the first being the upper left (x,y) coordinate, and the second being the lower right (x,y) coordinate
        """
        
        # Build file path to shapefile, and if it doesn't exist go ahead and download it
        shape_path = os.path.join(self.data_dir, SHAPEFILE_FOLDER_NAME, shapefile, shapefile + ".shp")
        if not os.path.exists(shape_path):
            self.__download_shapefiles(shapefile)
        
        ds = ogr.Open(shape_path)
        layer = ds.GetLayer()
        ext = layer.GetExtent()
        upper_left = (ext[0], ext[3])
        lower_right = (ext[1], ext[2])
    
        return upper_left, lower_right 


    def fetch_dem(self, shapefile=None, bbox={"xmin": -84.0387, "ymin": 35.86, "xmax": -83.815, "ymax": 36.04}, dataset="30m", prod_format="GeoTIFF", txt_file="download_urls", save_to_txt=True, download_folder="dem_tiles", download=False):
        """
        Queries USGS API for DEM data (URLs) given specified parameters.

        This function targets USGS National Map API to fetch DEM data URLs using specified parameters and can either 
        save the list of URLs to a text file or download from the list of URLs immediately. 
        GEOtiled currently only supports computation on GeoTIFF files, so it is not recommended to change the `prod_format` variable.
    
        Parameters
        ----------
        shapefile : str
            Code of shapefile with which a bounding box will be generated (default is None). Overrides the 'bbox' parameter if set.
        bbox : dict
            Bounding box coordinates to query for DEM data (default is {"xmin": -84.0387, "ymin": 35.86, "xmax": -83.815, "ymax": 36.04}).
        dataset : str
            Code for DEM data to download (default is 30m).
        prod_format : str
            File type of DEM data to download (default is GeoTIFF).
        txt_file : str
            Name of text file to save URLs to (default is download_urls).
        save_to_txt : bool
            Allow DEM URLs to be saved to a text file (default is True).
        download_folder : str
            Name of the download folder (default is dem_tiles).
        download : bool
            Allow DEM URLs retrieved to be immediately downloaded (default is False).

        Raises
        ------
        Exception
            If an invalid response while getting URLs off the USGS webpage, the function will error.
        """
        
        # Get coordinate extents if a shape file was specified
        if shapefile is not None:
            print("Reading in shape file...")

            # Get extents of shape file
            coords = self.__get_extents(shapefile)
            bbox["xmin"] = coords[0][0]
            bbox["ymax"] = coords[0][1]
            bbox["xmax"] = coords[1][0]
            bbox["ymin"] = coords[1][1]
    
        # Construct the query parameters
        print("Setting boundary extents...")
        data_codes = self.__get_codes("data")
        params = {
            "bbox": f"{bbox["xmin"]},{bbox["ymin"]},{bbox["xmax"]},{bbox["ymax"]}",
            "datasets": data_codes[dataset],
            "prodFormats": prod_format
        }
    
        # Make a GET request
        print("Requesting data from USGS...")
        base_url = "https://tnmaccess.nationalmap.gov/api/v1/products"
        response = requests.get(base_url, params=params)
    
        # Check for a successful request
        if response.status_code != 200:
            raise Exception(
                f"Failed to fetch data. Status code: {response.status_code}")
    
        # Convert JSON response to Python dict
        data = response.json()
    
        # Extract download URLs
        download_urls = [item["downloadURL"] for item in data["items"]]
    
        # Save URLs to text file 
        if save_to_txt is True:
            print("Saving URLs to text file...")
            txt_Path = os.path.join(self.data_dir, txt_file + TEXT_FILE_EXTENSION) # Build full path to text file
            with open(txt_Path, "w") as file:
                for url in download_urls:
                    file.write(f"{url}\n")
    
        # Download the files from the URLs
        if download is True:
            self.download_files(download_urls, download_folder)
    
        print("Fetch process complete.")


    def build_mosaic(self, input_folder, output_file, description, cleanup=False):
        """
        Builds a mosaic out of multiple GeoTIFF files.
        
        This function creates a mosaic from a list of GeoTIFF files utilizing the buildVRT() function from the GDAL library.
    
        Parameters
        ----------
        input_folder : str
            Name of the folder in data directory where files to mosaic together are located.
        output_file : str
            Name of mosaic file produced.
        description : str
            Description to add to output raster band of mosaic file.
        cleanup : bool, optional
            Determines if files from `input_folder` should be deleted after mosaic is complete (default is False).
        """
        
        # Create paths to files needed for computation
        vrt_path = os.path.join(self.data_dir, VRT_DEFAULT_FILE_NAME)
        mosaic_path = os.path.join(self.data_dir, output_file + GEOTIFF_FILE_EXTENSION)
        input_folder = os.path.join(self.data_dir, input_folder)
        
        # Check for valid folder and put input files into list
        print("Getting input files...")
        if not Path(input_folder).exists():
            print("The folder", input_folder, "does not exist. Terminating execution.")
            return
        input_files = glob.glob(input_folder + "/*" + GEOTIFF_FILE_EXTENSION)
    
        # Build VRT (mosaic)
        print("Constructing VRT...")
        vrt = gdal.BuildVRT(vrt_path, input_files)
        translate_options = gdal.TranslateOptions(creationOptions=["COMPRESS=LZW", "TILED=YES", "BIGTIFF=YES", "NUM_THREADS=ALL_CPUS"])
        gdal.Translate(mosaic_path, vrt, options=translate_options)
        vrt = None  # close file
    
        # Update band description with name of terrain parameter
        print("Updating band description...")
        dataset = gdal.Open(mosaic_path)
        band = dataset.GetRasterBand(1)
        band.SetDescription(description)
        dataset = None  # close file
    
        # Delete intermediary tiles used to build mosaic
        if cleanup is True:
            print("Cleaning intermediary files...")
            shutil.rmtree(input_folder)
        os.remove(vrt_path)
    
        print("Mosaic process complete.")


    def reproject(self, input_file, output_file, projection, cleanup=False):
        """
        Reprojects a GeoTIFF file to a specified projection.
        
        This function reprojects a specified GeoTIFF file to a new projection changing the coordinate system representation, 
        and the result is saved to a new file. Multithreading is utilized to improve performance.
    
        Parameters
        ----------
        input_file : str
            Name of GeoTIFF file in data directory to reproject.
        output_file : str
            Name of new reprojected GeoTIFF file to store in data directory.
        projection : str
            Projection to use for reprojection. Can be a EPSG code (e.g. EPSG:4326) or the path to a WKT file.
        cleanup : bool, optional
            Determines if `input_file` should be deleted after reprojection is complete (default is False).
        """
        
        # Set full file paths for input and output
        input_path = os.path.join(self.data_dir, input_file + GEOTIFF_FILE_EXTENSION)
        output_path = os.path.join(self.data_dir, output_file + GEOTIFF_FILE_EXTENSION)  
    
        # Ensure file to reproject exists
        if not os.path.isfile(input_path):
            print(os.path.basename(input_path), "does not exist. Terminating execution.")
            return
        
        print("Reprojection of", os.path.basename(input_path), "has begun.")
        
        # Set warp options for reprojection and warp
        warp_options = gdal.WarpOptions(dstSRS=projection, creationOptions=["COMPRESS=LZW", "TILED=YES", "BIGTIFF=YES", "NUM_THREADS=ALL_CPUS"],
                                        multithread=True, warpOptions=["NUM_THREADS=ALL_CPUS"])
        warp = gdal.Warp(output_path, input_path, options=warp_options)
        warp = None  # Close file
    
        # Delete files used for reprojection
        if cleanup is True:
            print("Cleaning intermediary files...")
            os.remove(input_path)
            os.remove(input_path + ".aux.xml")
    
        print("Reprojection process complete.")


    def __crop_pixels(self, input_file, output_file, window):
        """
        Crops a raster file to a specific region given a specified window.
        
        This function uses GDAL functions to crop data based off pixel coordinates rather than geospatial coordinates.
    
        Parameters
        ----------
        input_file : str
            Name of input file to be cropped.
        output_file : str
            Name of cropped output file.
        window : List[int], Tuple(int)
            List or tuple of format [left_x, top_y, width, height] where left_x and top_y are pixel coordinates of the upper-left corner 
            of the cropping window, and width and height specify the dimensions of the cropping window in pixels.
        """
        
        # Set full file paths for input and output
        input_path = os.path.join(self.data_dir, input_file + GEOTIFF_FILE_EXTENSION)
        output_path = os.path.join(self.data_dir, output_file + GEOTIFF_FILE_EXTENSION)  
        
        # Window to crop by [left_x, top_y, width, height]
        translate_options = gdal.TranslateOptions(srcWin=window, creationOptions=["COMPRESS=LZW", "TILED=YES", "BIGTIFF=YES"])

        # Perform translation
        gdal.Translate(output_path, input_path, options=translate_options)


    def crop_into_tiles(self, input_file, output_folder, num_tiles, buffer=10):
        """
        Splits a GeoTIFF file into smaller, equally-sized tiles.
        
        This function divides a GeoTIFF file into a specified number of tiles with added buffer regions to assist with rapid 
        computation of parameters by GEOtiled.
    
        Parameters 
        ----------
        input_file : str
            Name of the GeoTIFF file in the data directory to crop.
        output_folder : str
            Name of the folder in the data directory to store the cropped tiles.
        n_tiles : int 
            Number of total tiles to produce. Should be a perfect square number.
        buffer : int
            Specifies the buffer size - overlapping pixels that is included in the borders between two tiles (default is 10).
        """
        
        # Update path to input file
        input_path = os.path.join(self.data_dir, input_file + GEOTIFF_FILE_EXTENSION)
    
        # Ensure file to crop exists
        if not os.path.isfile(input_path):
            print(os.path.basename(input_path), "does not exist. Terminating execution.")
            return
        
        # Update path to out folder and create it if not done so
        output_path = os.path.join(self.data_dir, output_folder)
        Path(output_path).mkdir(parents=True, exist_ok=True)
        
        # Square root number of tiles to help get even number of rows and columns
        num_tiles = math.sqrt(num_tiles)
    
        # Split rows and columns of original file into even number of pixels for total number of tiles specified
        ds = gdal.Open(input_path, 0)
        cols = ds.RasterXSize
        rows = ds.RasterYSize
        x_win_size = int(math.ceil(cols / num_tiles))
        y_win_size = int(math.ceil(rows / num_tiles))
    
        tile_count = 0 # Track number of tiles cropped
    
        for i in range(0, rows, y_win_size):
            if i + y_win_size < rows:
                nrows = y_win_size
            else:
                nrows = rows - i
    
            for j in range(0, cols, x_win_size):
                if j + x_win_size < cols:
                    ncols = x_win_size
                else:
                    ncols = cols - j
    
                tile_file = output_path + "/tile_" + "{0:04d}".format(tile_count) + GEOTIFF_FILE_EXTENSION
                win = [j, i, ncols, nrows]
    
                # Upper left corner
                win[0] = max(0, win[0] - buffer)
                win[1] = max(0, win[1] - buffer)
    
                w = win[2] + 2 * buffer
                win[2] = w if win[0] + w < cols else cols - win[0]
    
                h = win[3] + 2 * buffer
                win[3] = h if win[1] + h < rows else rows - win[1]  
    
                self.__crop_pixels(input_file, os.path.join(output_folder, os.path.basename(tile_file).replace(GEOTIFF_FILE_EXTENSION, "")), win)
                print(os.path.basename(tile_file), "cropped.")
                tile_count += 1


    def __compute_params(self, input_path, param_list):
        """
        Computes terrain parameters for a given elevation GeoTIFF file.
        
        This function utilizes GDAL and GRASS libraries to compute parameters like slope, aspect, and more for a GeoTIFF file containing elevation data.
        The full parameter list can be found in the 'parameter_codes.txt' file.
    
        Parameters 
        ----------
        input_path : str
            Path to a GeoTIFF elevation file to compute parameters with.
        param_list : List[str]
            List of valid parameters to compute. The prefix for folder names should be the last item in the list.
        """
        
        # Get the name of the elevation file
        input_file = os.path.basename(input_path)
    
        # Compute all terrain parameters from passed in list
        for param in param_list:
            if param != param_list[-1]:
                # Set directory where computed tile will be stored
                param_path = os.path.join(self.data_dir, param_list[-1] + param + "_tiles")
                Path(param_path).mkdir(parents=True, exist_ok=True)
                output_path = os.path.join(param_path, input_file)
                
                # Set correct options and compute parameter
                if param in ["slope", "aspect", "hillshade"]:
                    if param == "aspect":
                        dem_options = gdal.DEMProcessingOptions(zeroForFlat=False, format="GTiff", creationOptions=["COMPRESS=LZW", "TILED=YES", "BIGTIFF=YES"])
                        gdal.DEMProcessing(output_path, input_path, processing=param, options=dem_options)
                    else:
                        dem_options = gdal.DEMProcessingOptions(format="GTiff", creationOptions=["COMPRESS=LZW", "TILED=YES", "BIGTIFF=YES"])
                        gdal.DEMProcessing(output_path, input_path, processing=param, options=dem_options)
                # else:
                #     # Define where to process the data in the temporary grass-session
                #     tmpdir = tempfile.TemporaryDirectory()
        
                #     # Create GRASS session
                #     s = Session()
                #     s.open(gisdb=tmpdir.name, location="PERMANENT", create_opts=input_path)
                #     creation_options = "BIGTIFF=YES,COMPRESS=LZW,TILED=YES" # For GeoTIFF files
                    
                #     # Load raster into GRASS without loading it into memory (else use r.import or r.in.gdal)
                #     gscript.run_command("r.external", input=input_path, output="elevation", overwrite=True, quiet = True)
                    
                #     # Set output folder for computed parameters
                #     gscript.run_command("r.external.out", directory=param_path, format="GTiff", option=creation_options, quiet = True)
        
                #     # Compute parameter
                #     if param == "topographic_wetness_index":
                #         gscript.run_command("r.topidx", input="elevation", output=input_file, overwrite=True, quiet = True)
                #     elif param == "plan_curvature":
                #         gscript.run_command("r.slope.aspect", elevation="elevation", tcurvature=input_file, flags="e", overwrite=True, quiet = True)
                #     elif param == "profile_curvature":
                #         gscript.run_command("r.slope.aspect", elevation="elevation", pcurvature=input_file, flags="e", overwrite=True, quiet = True)
                #     elif param == "convergence_index":
                #         gscript.run_command("r.convergence", input="elevation", output=input_file, overwrite=True, quiet = True) #addon
                #     elif param == "valley_depth":
                #         gscript.run_command("r.valley.bottom", input="elevation", mrvbf=input_file, overwrite=True, quiet = True) #addon
                #     elif param == "ls_factor":
                #         gscript.run_command("r.watershed", input="elevation", length_slope=input_file, threshold=1, overwrite=True, quiet = True)
                    
                #     # Cleanup
                #     tmpdir.cleanup()
                #     s.close()
            
                # Update band description and nodata value (for GRASS params)
                dataset = gdal.Open(output_path)
                band = dataset.GetRasterBand(1)
                band.SetDescription(param)
                if param not in ["slope", "aspect", "hillshade"]:
                    band.SetNoDataValue(-9999)
                dataset = None

        print("Computation of parameters for", input_file.replace(GEOTIFF_FILE_EXTENSION,""), "completed.")


    def compute_geotiled(self, input_folder, param_list, num_procs, output_folder_prefix='', cleanup=False):
        """
        Configures the multiprocessing pool for GEOtiled to begin computing terrain parameters.
        
        This function utilizes the multiprocessing library to allow for computation of parameters on different elevation GeoTIFF files at the same time.
        Parameters that can be computed are specified in the 'parameter_codes.txt', or the 'all' keyword can be passed to compute all parameters.
        It is better to keep `num_procs` as a low value for systems with low amounts of RAM.
        
        Parameters
        ----------
        input_folder : str
            Name of the folder in the data directory containing DEM elevation files.
        param_list : List[str]
            List containing codes for terrain parameters to compute. The 'all' keyword will compute all params.
        num_procs : int
            Integer specifying the number of python instances to use for multiprocessing.
        output_folder_prefix : str
            Prefix to attach to all output folders created for storing computed terrain paramters (default is '').
        cleanup : bool
            Determine if elevation files should be deleted after computation (default is False).
        """
        
        # Check to ensure input folder exists
        input_path = os.path.join(self.data_dir, input_folder)
        if not Path(input_path).exists():
            print("The folder", input_path, "does not exist. Terminating execution.")
            return
        
        # Check if all params are to be computed
        param_codes = self.__get_codes("parameter")
        if "all" in param_list:
            # Ensure only the 'all' code entered
            if len(param_list) > 1:
                print("Can't use 'all' keyword with other parameters in list")
                return
            
            param_list.remove("all")
            for key in param_codes:
                param_list.append(key)
    
        # Ensure all codes entered are valid codes and put full parameter name in different list
        params = []
        for param in param_list:
            if param not in list(param_codes.keys()):
                print("Parameter code", param, "is not a valid code")
                return
            params.append(param_codes[param])
    
        # Append the output_folder_prefix value to the params list to ensure it gets passed into the processing pool
        if output_folder_prefix != '':
            output_folder_prefix = output_folder_prefix + '_'
        params.append(output_folder_prefix)
        
        # Get files from input folder to compute on 
        print('Getting input files...')
        input_files = sorted(glob.glob(input_path + '/*' + GEOTIFF_FILE_EXTENSION))
    
        # Configure a list with the input files and selected params to support computing with pool.starmap()
        # https://superfastpython.com/multiprocessing-pool-starmap/
        items = []
        for input_file in input_files:
            items.append((input_file, params))
    
        # Create multiprocessing pool based off number of tiles to compute and compute params
        print('Starting computation of parameters...')
        pool = multiprocessing.Pool(processes=num_procs) 
        pool.starmap(self.__compute_params, items)
    
        # Remove files used to compute params
        if cleanup is True:
            print('Cleaning files...')
            shutil.rmtree(input_path)
    
        print('GEOtiled computation done!')